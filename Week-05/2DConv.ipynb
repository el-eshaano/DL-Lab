{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39bb6269-54a0-42b2-b819-e108f45cc0aa",
   "metadata": {},
   "source": [
    "## Q1 - F.conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a69c712a-479b-4a4e-a894-4822aef6c00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.5346, 0.7714, 0.3719, 0.8360, 0.5781, 0.6845],\n",
      "          [0.2050, 0.7051, 0.1959, 0.6867, 0.4436, 0.5854],\n",
      "          [0.4251, 0.7226, 0.6444, 0.6268, 0.2365, 0.4054],\n",
      "          [0.4202, 0.4937, 0.8623, 0.6452, 0.0246, 0.9901],\n",
      "          [0.0829, 0.9093, 0.7794, 0.5133, 0.8842, 0.6364],\n",
      "          [0.0800, 0.8497, 0.4085, 0.8312, 0.5820, 0.9909]]]])\n",
      "torch.Size([1, 1, 6, 6])\n",
      "---\n",
      "tensor([[[[1.5648, 2.2782, 1.4313, 2.0411],\n",
      "          [1.9578, 2.2583, 1.3454, 2.2665],\n",
      "          [2.0126, 2.2678, 2.1763, 2.0761],\n",
      "          [1.6518, 2.5919, 2.1477, 2.7438]]]])\n",
      "torch.Size([1, 1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "image = torch.rand(1, 1, 6, 6)\n",
    "print(image)\n",
    "print(image.shape)\n",
    "print('---')\n",
    "\n",
    "kernel = torch.rand(1, 1, 3, 3)\n",
    "\n",
    "outimage = F.conv2d(image, kernel, stride=1, padding=0)\n",
    "print(outimage)\n",
    "print(outimage.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bba594a-3a2a-4c37-b16b-f65bd4ecf58f",
   "metadata": {},
   "source": [
    "## Q2 - nn.Conv2D vs F.conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca48a95b-dcf8-42ec-a8be-c1a4e5e1c664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Image\n",
      "---\n",
      "tensor([[[[0.1963, 0.5757, 0.3885, 0.7981, 0.2324, 0.1389],\n",
      "          [0.5888, 0.1574, 0.1147, 0.6467, 0.4757, 0.8957],\n",
      "          [0.4251, 0.3464, 0.8903, 0.1398, 0.2894, 0.2301],\n",
      "          [0.8477, 0.5820, 0.8107, 0.9635, 0.4911, 0.1770],\n",
      "          [0.2641, 0.6290, 0.7383, 0.2848, 0.8211, 0.0257],\n",
      "          [0.8565, 0.2940, 0.5266, 0.9157, 0.9420, 0.7643]]]])\n",
      "torch.Size([1, 1, 6, 6])\n",
      "---\n",
      "Outputs\n",
      "---\n",
      "tensor([[[[-0.0608, -0.1556,  0.0898,  0.0770],\n",
      "          [ 0.2823,  0.1176, -0.3721, -0.0634],\n",
      "          [ 0.0163, -0.1133,  0.2568, -0.4145],\n",
      "          [ 0.1980, -0.0814, -0.1377, -0.0283]],\n",
      "\n",
      "         [[-0.5834, -0.3468, -0.5163, -0.4446],\n",
      "          [-0.7003, -0.8488, -0.7547, -0.3851],\n",
      "          [-0.6710, -0.8057, -0.8550, -0.6293],\n",
      "          [-0.5483, -0.7581, -0.8762, -0.9860]],\n",
      "\n",
      "         [[-0.1004,  0.0658,  0.1614,  0.1502],\n",
      "          [-0.1830,  0.0450, -0.1704,  0.3452],\n",
      "          [ 0.0906, -0.0585,  0.3177,  0.0575],\n",
      "          [ 0.3370,  0.2679, -0.1367,  0.1809]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "torch.Size([1, 3, 4, 4])\n",
      "---\n",
      "tensor([[[[-0.0608, -0.1556,  0.0898,  0.0770],\n",
      "          [ 0.2823,  0.1176, -0.3721, -0.0634],\n",
      "          [ 0.0163, -0.1133,  0.2568, -0.4145],\n",
      "          [ 0.1980, -0.0814, -0.1377, -0.0283]],\n",
      "\n",
      "         [[-0.5834, -0.3468, -0.5163, -0.4446],\n",
      "          [-0.7003, -0.8488, -0.7547, -0.3851],\n",
      "          [-0.6710, -0.8057, -0.8550, -0.6293],\n",
      "          [-0.5483, -0.7581, -0.8762, -0.9860]],\n",
      "\n",
      "         [[-0.1004,  0.0658,  0.1614,  0.1502],\n",
      "          [-0.1830,  0.0450, -0.1704,  0.3452],\n",
      "          [ 0.0906, -0.0585,  0.3177,  0.0575],\n",
      "          [ 0.3370,  0.2679, -0.1367,  0.1809]]]])\n",
      "torch.Size([1, 3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print('---')\n",
    "print('Image')\n",
    "print('---')\n",
    "image = torch.rand(1, 1, 6, 6)\n",
    "print(image)\n",
    "print(image.shape)\n",
    "print('---')\n",
    "print('Outputs')\n",
    "print('---')\n",
    "\n",
    "m = nn.Conv2d(1, 3, 3, stride=1, padding=0, bias=False)\n",
    "outimage_nn = m(image)\n",
    "\n",
    "k = [n.data for n in m.parameters()][0]\n",
    "outimage_F = F.conv2d(image, k, stride=1, padding=0)\n",
    "\n",
    "\n",
    "print(outimage_nn)\n",
    "print(outimage_nn.size())\n",
    "print('---')\n",
    "print(outimage_F)\n",
    "print(outimage_F.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ebc8ff-e4e9-4ed0-8959-0338e335fecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
